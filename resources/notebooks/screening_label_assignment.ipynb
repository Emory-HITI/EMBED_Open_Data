{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988cb892-2bff-49e1-be59-b21ffeed60ee",
   "metadata": {},
   "source": [
    "# Magview Screening Label Assignment Workflow \n",
    "## *[Public Release]*\n",
    "### Beatrice Brown-Mulry\n",
    "### 02/27/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb105b-4c8c-493b-a1f5-425cbc3ed6bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<style>\n",
    "    h4, h5 {\n",
    "        margin: 0;\n",
    "        padding: 0.5rem;\n",
    "        font-weight: normal;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<a id='contents'></a>\n",
    "\n",
    "## Contents\n",
    "- #### [0. Data Preparation](#section-0)\n",
    "    - ##### [0.1 Prepare Magview](#section-0-1)\n",
    "    - ##### [0.2 Correct Contralaterals](#section-0-2)\n",
    "    - ##### [0.3 Derive Exam Laterality](#section-0-3)\n",
    "    - ##### [0.4 Derive Exam-Level BIRADS and Path.](#section-0-4)\n",
    "    - ##### [0.5 Aggregate Exam Biopsy Sides](#section-0-5)\n",
    "    - ##### [0.6 Handle Addended v1 Exam Data](#section-0-6)\n",
    "    - ##### [0.7 Assign Screen BIRADS Helper Variables](#section-0-7)\n",
    "    - ##### [0.8 Prepare Target Sample Subset](#section-0-8)\n",
    "- #### [1. Handle Follow-Up Exam Data](#section-1)\n",
    "    - ##### [1.1 Define Diagnostic/Ultrasound Dataframes](#section-1-1)\n",
    "    - ##### [1.2 Get Follow-Up Mappings](#section-1-2)\n",
    "    - ##### [1.3 Perform Follow-Up Mapping](#section-1-3)\n",
    "    - ##### [1.4 Summarize Follow-Ups](#section-1-4)\n",
    "    - ##### [1.5 Assign Follow-Up Helper Variables](#section-1-5)\n",
    "    - ##### [1.6 Identify Interval Cancers](#section-1-6)\n",
    "    - ##### [1.7 Evaluate Long-Term Follow-Up Status](#section-1-7)\n",
    "- #### [2. Data Finalization](#section-2)\n",
    "    - ##### [2.1 Data Enrichment](#section-2-1)\n",
    "    - ##### [2.2 Data Cleaning](#section-2-2)\n",
    "    - ##### [2.3 Finalize and Output Data](#section-2-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b8142c-8a26-4b88-9740-bcee5353af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import embed_toolkit # version 0.2.*\n",
    "from dotenv import load_dotenv # this is only used in this example to load filepaths from a .env file!\n",
    "from typing import Optional, Union\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "tqdm.pandas() # initialize tqdm wrapper for pandas.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeba4ce-6345-4550-93be-319d0b4d90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example I'll be loading my filepaths from a .env file with the dotenv package\n",
    "# but these should be changed as needed\n",
    "load_dotenv()\n",
    "MAGVIEW_PATH: str = os.environ['MAGVIEW_PATH']\n",
    "SCORE_PATH: str = os.environ['SCORE_PATH']\n",
    "OUTPUT_PATH: Optional[str] = os.environ.get('OUTPUT_PATH', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ee738",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0'></a>\n",
    "# 0. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cde75",
   "metadata": {},
   "source": [
    "<a id='section-0-1'></a>\n",
    "## 0.1 Prepare Magview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a2e97e6-4556-445e-937e-b1429d65c637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2507914/3739102562.py:2: DtypeWarning: Columns (5,6,12,17,19,22,23,39,41,43,44,45,46,47,48,49,50,51,52,53,54,55,57,58,59,60,61,62,63,64,65,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,91,104,106,107,109,110,111,112,113,121,123,124,141,142,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mag_df = pd.read_csv(MAGVIEW_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Magview         \n",
      "┌───────────┬──────────┐\n",
      "│ Feature   │ Count    │\n",
      "├───────────┼──────────┤\n",
      "│ Patients  │ 116,597  │\n",
      "│ Exams     │ 678,655  │\n",
      "│ Findings  │ 748,803  │\n",
      "└───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# load dataframe\n",
    "mag_df = pd.read_csv(MAGVIEW_PATH)\n",
    "\n",
    "# ensure key columns have the correct data types\n",
    "mag_df['empi_anon'] = pd.to_numeric(mag_df['empi_anon'])\n",
    "mag_df['acc_anon'] = pd.to_numeric(mag_df['acc_anon'])\n",
    "mag_df['study_date_anon'] = pd.to_datetime(mag_df['study_date_anon'])\n",
    "\n",
    "# create a helper column for exam screen-status\n",
    "mag_df['screen_exam'] = mag_df.desc.str.contains('screen', case=False)\n",
    "\n",
    "# summarize dataframe contents\n",
    "mag_df.embed.summarize(\"Magview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd312d",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-2'></a>\n",
    "## 0.2 Correct Contralaterals\n",
    "We need to run the contralateral correction function on our Magview dataframe to create entries for any negative contralateral findings that are implied on bilateral exams with single-sided unilateral findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a09638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cef26f0899468ab38b2147ee5495e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbrow51/.conda/envs/dsci/lib/python3.11/site-packages/embed_toolkit/magview.py:560: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return concat([out_df, correction_df]).sort_values(['empi_anon', 'acc_anon', 'numfind']).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# apply contralateral correction, drop any data for exams with no description\n",
    "mag_df = mag_df.dropna(subset=\"desc\")\n",
    "mag_contra_df = embed_toolkit.correct_contralaterals(mag_df)\n",
    "\n",
    "# correct column dtypes\n",
    "mag_contra_df['study_date_anon'] = pd.to_datetime(mag_contra_df['study_date_anon'])\n",
    "mag_contra_df['acc_anon'] = pd.to_numeric(mag_contra_df['acc_anon'])\n",
    "mag_contra_df['empi_anon'] = pd.to_numeric(mag_contra_df['empi_anon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dfe67e",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-3'></a>\n",
    "## 0.3 Derive Exam Laterality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f311cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 893670/893670 [00:07<00:00, 114511.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_exam_laterality(row: pd.Series) -> str | None:\n",
    "    # extract description and lowercase it\n",
    "    finding_desc = row.desc.lower()\n",
    "    \n",
    "    if (\"bilat\" in finding_desc):\n",
    "        return \"B\"\n",
    "    elif (\"left\" in finding_desc):\n",
    "        return \"L\"\n",
    "    elif (\"right\" in finding_desc):\n",
    "        return \"R\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# derive exam laterality from their descriptions\n",
    "mag_contra_df[\"exam_laterality\"] = mag_contra_df.progress_apply(get_exam_laterality, axis=1) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d1053",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-4'></a>\n",
    "## 0.4 Derive Exam-Level BIRADS and Path.\n",
    "Get the most severe BIRADS and path_severity associated with each exam to use as its representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each screening exam, take the most severe birads as the representative\n",
    "def get_worst_ps(group):\n",
    "    return group.path_severity.min()\n",
    "\n",
    "def get_worst_br(group):\n",
    "    exam_desc = group.desc.tolist()[0]\n",
    "    if \"screen\" in exam_desc.lower():\n",
    "        br_to_val_dict = {\n",
    "            'A': 0, # 'A' maps to birads 0\n",
    "            'B': 1, # 'B' maps to birads 2\n",
    "            'N': 2  # 'N' maps to birads 1\n",
    "        }\n",
    "    else:\n",
    "        br_to_val_dict = {\n",
    "            'N': 5, # 'N' maps to birads 1\n",
    "            'B': 4, # 'B' maps to birads 2\n",
    "            'P': 3, # 'P' maps to birads 3\n",
    "            'S': 2, # 'S' maps to birads 4\n",
    "            'M': 1, # 'M' maps to birads 5\n",
    "            'K': 0  # 'K' maps to birads 6\n",
    "        }\n",
    "        \n",
    "    val_to_br_dict = {v:k for k,v in br_to_val_dict.items()}\n",
    "    worst_br_val = min(group.asses.map(br_to_val_dict).tolist())\n",
    "    return val_to_br_dict.get(worst_br_val, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a355b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 678651/678651 [01:48<00:00, 6258.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the 'get_worst_br' function to the data (grouped by exam) and output [exam > birads] mappings as a dict\n",
    "worst_br_dict = mag_contra_df.groupby('acc_anon').progress_apply(get_worst_br).to_dict() # type: ignore\n",
    "\n",
    "# map back to magview\n",
    "mag_contra_df['exam_birads'] = ''\n",
    "mag_contra_df['exam_birads'] = mag_contra_df['acc_anon'].map(worst_br_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06b3221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31758/31758 [00:01<00:00, 28708.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the 'get_worst_ps' function to the data (grouped by exam) and output [exam > pathology] mappings as a dict\n",
    "# don't apply it to exam findings with no path severity (since they can't affect results)\n",
    "worst_path_dict = mag_contra_df[~pd.isnull(mag_contra_df.path_severity)].groupby('acc_anon').progress_apply(get_worst_ps).to_dict() # type: ignore\n",
    "\n",
    "# map back to magview\n",
    "mag_contra_df['exam_path_severity'] = np.nan\n",
    "mag_contra_df['exam_path_severity'] = mag_contra_df['acc_anon'].map(worst_path_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807dce4",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-5'></a>\n",
    "## 0.5 Aggregate Exam Biopsy Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cf2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 678651/678651 [00:19<00:00, 35272.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "exam_bside\n",
       "NaN    826027\n",
       "L       33193\n",
       "R       32518\n",
       "B        1932\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_bsides(group):\n",
    "    # applied to exam groups\n",
    "    bside_list = group.bside.unique().tolist()\n",
    "    # return the only bside if we only have 1 (this should never be 0 since NaN is included)\n",
    "    # this should return an IndexError if it ever is 0\n",
    "    if len(bside_list) == 1:\n",
    "        return bside_list[0]\n",
    "\n",
    "    # otherwise aggregate bilateral bsides\n",
    "    elif ('B' in bside_list) or (('L' in bside_list) & ('R' in bside_list)):\n",
    "        return 'B'\n",
    "    # handle left bsides with no right or 'B' (other is a NaN)\n",
    "    elif ('L' in bside_list):\n",
    "        return 'L'\n",
    "    # handle right bsides with no left or 'B' (other is a NaN)\n",
    "    elif ('R' in bside_list):\n",
    "        return 'R'\n",
    "    else:\n",
    "        return 'ERROR'\n",
    "\n",
    "def get_bside_aggregation_dict(df: pd.DataFrame) -> dict[float, str]:\n",
    "    # we only need to apply this to exam findings with no exam-level pathology registered\n",
    "    path_na_mask: pd.Series[bool] = pd.isna(mag_contra_df.exam_path_severity)\n",
    "\n",
    "    # or exam findings where the finding-level path severity matches the exam-level path severity\n",
    "    path_match_mask: pd.Series[bool] = (\n",
    "        ~pd.isna(mag_contra_df.exam_path_severity)\n",
    "        & (mag_contra_df.path_severity == mag_contra_df.exam_path_severity)\n",
    "    )\n",
    "\n",
    "    # define a list of columns to consider\n",
    "    col_list: list[str] = ['acc_anon', 'empi_anon', 'study_date_anon', 'exam_birads', 'exam_path_severity', 'bside']\n",
    "\n",
    "    # get the relevant subset of the data, then \n",
    "    df_subset: pd.DataFrame = df.loc[path_na_mask | path_match_mask, col_list]\n",
    "\n",
    "    # drop any duplicate rows, group by exam, then apply the agg func and output a [exam > bside] mapping dict\n",
    "    bside_agg_dict: dict[float, str] = (\n",
    "        df_subset\n",
    "        .drop_duplicates()\n",
    "        .groupby('acc_anon')\n",
    "        .progress_apply(aggregate_bsides) # type: ignore\n",
    "        .to_dict()\n",
    "    )\n",
    "    return bside_agg_dict\n",
    "\n",
    "# apply the agg function and get a dict of exam mappings\n",
    "bside_agg_dict: dict[float, str] = get_bside_aggregation_dict(mag_contra_df)\n",
    "\n",
    "# map the agg dict back to the dataframe\n",
    "mag_contra_df['exam_bside'] = mag_contra_df['acc_anon'].map(bside_agg_dict)\n",
    "mag_contra_df['exam_bside'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac2c0b",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-6'></a>\n",
    "## 0.6 Handle Addended v1 Exam Data\n",
    "In some cases in EMBED v1, exams that were addended at some point in time have incomplete information in our data. This is one example where that's observable: after manually reviewing all of our completely-negative exams with a pathology result linked *directly* to a negative finding in an exam, we found they had all been addended to BIRADS A at a later date. We'll manually correct these to BIRADS A exams for now so they get handled correctly by the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20798cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct exam birads to 'A' for any 'negative' exams with a pathology assigned directly to the screen findings\n",
    "# a review of the associated rad. notes indicated these were all addended to 'A's\n",
    "addended_exam_list = mag_contra_df[\n",
    "    (mag_contra_df.screen_exam == True) \n",
    "    & mag_contra_df.exam_birads.isin(['N', 'B']) \n",
    "    & ~pd.isna(mag_contra_df.exam_path_severity)\n",
    "].acc_anon.unique().tolist()\n",
    "\n",
    "mag_contra_df.loc[mag_contra_df.acc_anon.isin(addended_exam_list), 'exam_birads'] = 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bf5034",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-7'></a>\n",
    "## 0.7 Assign Screen BIRADS Helper Variables\n",
    "These are a shorthand for exams with a screening `exam_birads` of 'A' (abnormal) or 'N'/'B' (negative/benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f0fe482",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_br_0_list = mag_contra_df[\n",
    "    (mag_contra_df.screen_exam == True)\n",
    "    & (mag_contra_df.exam_birads.isin(['A']))\n",
    "].acc_anon.unique().tolist()\n",
    "\n",
    "scr_br_12_list = mag_contra_df[\n",
    "    (mag_contra_df.screen_exam == True)\n",
    "    & (mag_contra_df.exam_birads.isin(['N', 'B']))\n",
    "].acc_anon.unique().tolist()\n",
    "\n",
    "mag_contra_df['scr_br_0'] = False\n",
    "mag_contra_df.loc[mag_contra_df.acc_anon.isin(scr_br_0_list), 'scr_br_0'] = True\n",
    "\n",
    "mag_contra_df['scr_br_12'] = False\n",
    "mag_contra_df.loc[mag_contra_df.acc_anon.isin(scr_br_12_list), 'scr_br_12'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea03987-21e2-4178-b90c-2510745caf31",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-0-8'></a>\n",
    "## 0.8 Prepare Target Sample Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee2d08-16b9-45e8-b318-ad712a9f0dd1",
   "metadata": {},
   "source": [
    "In this workflow, we'll identify a subset of our data which will be the *actual* target for label assignment. Splitting our data into two like this allows us to consider information from *all* exams when processing a subset (for example, we might only want to work with screening images but we still want to consider all events that occured for a patient). \n",
    "\n",
    "Here, we'll define a subset by merging in a dataframe with mappings between the exam ID column, `acc_anon`, and predicted malignancy scores from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85285a6b-e4fb-4a62-bb37-c223436d38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_score_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # correct variable dtypes and rename columns\n",
    "    rename_dict: dict[str, str] = {\n",
    "        'accession number': 'acc_anon',\n",
    "        'Cohort': 'cohort'\n",
    "    }\n",
    "\n",
    "    data.rename(columns=rename_dict, inplace=True)\n",
    "    return data[['acc_anon', 'cohort', 'score']]\n",
    "\n",
    "\n",
    "# load our score dataframe and prepare it\n",
    "score_df: pd.DataFrame = pd.read_csv(SCORE_PATH)\n",
    "score_df = prepare_score_data(score_df)\n",
    "\n",
    "# ensure key columns have the correct data types\n",
    "score_df['acc_anon'] = pd.to_numeric(score_df['acc_anon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeaed41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of accessions used in the study then get the subset of magview corresponding to it\n",
    "lunit_acc_list = score_df.acc_anon.tolist()\n",
    "mag_sample_df = mag_contra_df[mag_contra_df.acc_anon.isin(lunit_acc_list)]\n",
    "\n",
    "# we only want to consider screening exams in our target subset, so we'll drop any diagnostic cases present\n",
    "mag_sample_df = mag_sample_df[mag_sample_df.desc.str.contains('screen', case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f4063-1d23-436f-a44c-f6a237d731bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1'></a>\n",
    "# 1. Handle Follow-Up Exam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ba365",
   "metadata": {},
   "source": [
    "<a id='section-1-1'></a>\n",
    "## 1.1 Define Diagnostic/Ultrasound Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fca3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_cols = ['acc_anon', 'empi_anon', 'study_date_anon', 'exam_laterality', 'exam_birads', 'exam_path_severity', 'exam_bside']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95adc2da-a221-4355-90ea-f8f798fdef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any duplicate exam rows? False\n"
     ]
    }
   ],
   "source": [
    "# get subset of magview corresponding to diagnostic exams\n",
    "mag_diag = mag_contra_df.loc[mag_contra_df.desc.str.contains('diag', case=False)]\n",
    "mag_diag = mag_diag[followup_cols].drop_duplicates()\n",
    "\n",
    "# ensure we have exactly 1 row for each exam\n",
    "print('any duplicate exam rows?', mag_diag.acc_anon.nunique() != len(mag_diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19795563-56d2-44e0-b2b1-1ab7230dc4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any duplicate exam rows? False\n"
     ]
    }
   ],
   "source": [
    "# get subset of magview corresponding to ultrasound exams\n",
    "mag_us = mag_contra_df.loc[mag_contra_df.desc.str.contains('US')]\n",
    "mag_us = mag_us.dropna(subset=\"asses\")\n",
    "mag_us = mag_us[followup_cols].drop_duplicates()\n",
    "\n",
    "# ensure we have exactly 1 row for each exam\n",
    "print('any duplicate exam rows?', mag_us.acc_anon.nunique() != len(mag_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cd334-e496-4dbc-97f7-e8912f4532e3",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1-2'></a>\n",
    "## 1.2 Get Follow-Up Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e3e698-16e4-41dc-9f8c-676305494153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followup_map_dict(df: pd.DataFrame, followup_df: pd.DataFrame, time_delta: Union[int, float] = 180):\n",
    "    # don't consider followups with an undefined exam_birads (indicates an invalid birads for that stage)\n",
    "    # followup_df = followup_df[(followup_df.exam_birads != '') & ~pd.isna(followup_df.exam_birads)]\n",
    "    \n",
    "    # time delta in days\n",
    "    # expects df to have been corrected for contralateral findings (and for no NA finding sides to exist)\n",
    "    # previous versions assumed no 'B' findings but this does not\n",
    "    merge_df = df.merge(followup_df, on='empi_anon', how='inner', suffixes=(None, \"_fu\"))\n",
    "    \n",
    "    # ensure exam laterality match, L==L, R==R, or either original/followup is bilateral\n",
    "    merge_df = merge_df.loc[\n",
    "        (merge_df.exam_laterality==merge_df.exam_laterality_fu)\n",
    "        | (merge_df.exam_laterality==\"B\")\n",
    "        | (merge_df.exam_laterality_fu==\"B\")\n",
    "    ]\n",
    "\n",
    "    # exclude followups with an invalid time delta\n",
    "    merge_df[\"fu_delta\"] = (merge_df.study_date_anon_fu - merge_df.study_date_anon).dt.days\n",
    "    merge_df = merge_df.loc[(merge_df.fu_delta >= 0) & (merge_df.fu_delta <= time_delta)]\n",
    "\n",
    "    # get the accession of the first valid followup for each exam and output a dict of mappings\n",
    "    map_dict = merge_df.sort_values('fu_delta').drop_duplicates('acc_anon', keep='first').set_index('acc_anon')['acc_anon_fu'].to_dict()\n",
    "    return map_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7929b-e725-4d3c-b301-1bfc47dcad79",
   "metadata": {},
   "source": [
    "#### BIRADS 0 Exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b1c169-74f5-471e-ae13-1d44c7ee5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18281 valid DX followups found for Screen BIRADS 0s\n",
      "3454 valid US followups found for Screen BIRADS 0s\n"
     ]
    }
   ],
   "source": [
    "# mag_br0 = mag_contra_df[mag_contra_df.scr_br_0 == True]\n",
    "mag_br0 = mag_sample_df[mag_sample_df.scr_br_0 == True]\n",
    "\n",
    "# get birads 0 diagnostic followup map dict\n",
    "br0_dx_map_dict = get_followup_map_dict(mag_br0, mag_diag, time_delta=180)\n",
    "print(f\"{len(br0_dx_map_dict)} valid DX followups found for Screen BIRADS 0s\")\n",
    "\n",
    "# get birads 0 ultrasound followup map dict\n",
    "br0_us_map_dict = get_followup_map_dict(mag_br0, mag_us, time_delta=180)\n",
    "print(f\"{len(br0_us_map_dict)} valid US followups found for Screen BIRADS 0s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c60f9e-014b-4f02-b016-2c6ba108cb83",
   "metadata": {},
   "source": [
    "#### BIRADS 1/2 Exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edd19e9e-6c32-4f91-94bd-b3a43450fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2467 valid DX followups found for Screen BIRADS 1/2s\n",
      "416 valid US followups found for Screen BIRADS 1/2s\n"
     ]
    }
   ],
   "source": [
    "# mag_br12 = mag_contra_df[mag_contra_df.scr_br_12 == True]\n",
    "mag_br12 = mag_sample_df[mag_sample_df.scr_br_12 == True]\n",
    "\n",
    "# get birads 0 diagnostic followup map dict\n",
    "br12_dx_map_dict = get_followup_map_dict(mag_br12, mag_diag, time_delta=365)\n",
    "print(f\"{len(br12_dx_map_dict)} valid DX followups found for Screen BIRADS 1/2s\")\n",
    "\n",
    "# get birads 0 ultrasound followup map dict\n",
    "br12_us_map_dict = get_followup_map_dict(mag_br12, mag_us, time_delta=365)\n",
    "print(f\"{len(br12_us_map_dict)} valid US followups found for Screen BIRADS 1/2s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d86a6d-0b0a-4be4-9eaa-4ec208816f92",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1-3'></a>\n",
    "## 1.3 Perform Follow-Up Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc6789",
   "metadata": {},
   "source": [
    "#### Diagnostic Follow-Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "646868d9-abbc-4199-8b72-4b3d6ac0e913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine both dx map dicts and use it to derive a 'earliest_dx_acc' column in mag_sample_df\n",
    "# which contains any valid followup dx acc and exam-level dx birads/path severity\n",
    "mag_sample_df[\"earliest_dx_acc\"] = mag_sample_df[\"acc_anon\"].map({**br0_dx_map_dict, **br12_dx_map_dict})\n",
    "\n",
    "mag_sample_df = mag_sample_df.merge(\n",
    "    mag_diag[[\"acc_anon\", \"exam_birads\", \"exam_path_severity\", \"exam_bside\"]],\n",
    "    how=\"left\",\n",
    "    left_on=[\"earliest_dx_acc\"],\n",
    "    right_on=[\"acc_anon\"],\n",
    "    suffixes=(None, \"_dx\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a9e41-bd1e-4c85-ae92-98497904aa49",
   "metadata": {},
   "source": [
    "#### Ultrasound Follow-Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7666784-8856-4521-b013-cd48563cbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both dx map dicts and use it to derive a 'earliest_dx_acc' column in mag_sample_df\n",
    "# which contains any valid followup dx acc and exam-level dx birads/path severity\n",
    "mag_sample_df[\"earliest_us_acc\"] = mag_sample_df[\"acc_anon\"].map({**br0_us_map_dict, **br12_us_map_dict})\n",
    "\n",
    "mag_sample_df = mag_sample_df.merge(\n",
    "    mag_us[[\"acc_anon\", \"exam_birads\", \"exam_path_severity\", \"exam_bside\"]].drop_duplicates(),\n",
    "    how=\"left\",\n",
    "    left_on=[\"earliest_us_acc\"],\n",
    "    right_on=[\"acc_anon\"],\n",
    "    suffixes=(None, \"_us\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77c086-2ced-48b3-97dc-9677640f8f33",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1-4'></a>\n",
    "## 1.4 Summarize Follow-Ups\n",
    "Prioritize results on diagnostic follow-ups if they exist (and are more severe than existing valid ultrasound follow-ups). Otherwise, use any valid ultrasound follow-up results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cfdbc4b-dcd6-4360-811e-00836df687ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample_df[\"followup_type\"] = \"\"\n",
    "\n",
    "# present dx (prioritized if both dx and us present)\n",
    "mag_sample_df.loc[\n",
    "    ~pd.isna(mag_sample_df.earliest_dx_acc), \"followup_type\"\n",
    "] = \"DX\"\n",
    "\n",
    "# missing dx, present us\n",
    "mag_sample_df.loc[\n",
    "    pd.isna(mag_sample_df.earliest_dx_acc) \n",
    "    & ~pd.isna(mag_sample_df.earliest_us_acc), \"followup_type\"\n",
    "] = \"US\"\n",
    "\n",
    "# present dx + us, us path_severity more severe so it overrides dx\n",
    "mag_sample_df.loc[\n",
    "    ~pd.isna(mag_sample_df.earliest_dx_acc) \n",
    "    & ~pd.isna(mag_sample_df.earliest_us_acc)\n",
    "    & (mag_sample_df.exam_path_severity_us < mag_sample_df.exam_path_severity_dx), \"followup_type\"\n",
    "] = \"US\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c99e1c9-d9cc-4e45-9c68-c7267eb7d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample_df[\"followup_path_severity\"] = np.nan\n",
    "mag_sample_df.loc[mag_sample_df.followup_type == \"DX\", \"followup_path_severity\"] = mag_sample_df.loc[mag_sample_df.followup_type == \"DX\", \"exam_path_severity_dx\"]\n",
    "mag_sample_df.loc[mag_sample_df.followup_type == \"US\", \"followup_path_severity\"] = mag_sample_df.loc[mag_sample_df.followup_type == \"US\", \"exam_path_severity_us\"]\n",
    "\n",
    "mag_sample_df[\"followup_bside\"] = \"\"\n",
    "mag_sample_df.loc[mag_sample_df.followup_type == \"DX\", \"followup_bside\"] = mag_sample_df.loc[mag_sample_df.followup_type == \"DX\", \"exam_bside_dx\"]\n",
    "mag_sample_df.loc[mag_sample_df.followup_type == \"US\", \"followup_bside\"] = mag_sample_df.loc[mag_sample_df.followup_type == \"US\", \"exam_bside_us\"]\n",
    "\n",
    "mag_sample_df[\"followup_birads\"] = \"\"\n",
    "mag_sample_df.loc[mag_sample_df.followup_type == \"DX\", \"followup_birads\"] = mag_sample_df.loc[mag_sample_df.followup_type == \"DX\", \"exam_birads_dx\"]\n",
    "mag_sample_df.loc[mag_sample_df.followup_type == \"US\", \"followup_birads\"] = mag_sample_df.loc[mag_sample_df.followup_type == \"US\", \"exam_birads_us\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fc776-b2db-4c97-b6ff-23bef8bb305c",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1-5'></a>\n",
    "## 1.5 Assign Follow-Up Helper Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb1f93",
   "metadata": {},
   "source": [
    "#### BIRADS 1/2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f74e0617-990e-44bf-9912-5d0b6bcf2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample_df['dx_br_123'] = ''\n",
    "mag_sample_df.loc[(mag_sample_df.scr_br_0 == True), 'dx_br_123'] = False\n",
    "mag_sample_df.loc[(mag_sample_df.scr_br_0 == True) & (mag_sample_df.followup_birads.isin([\"N\", \"B\", \"P\"])), 'dx_br_123'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907f65d-11ae-4c93-933c-607ddb5d2771",
   "metadata": {},
   "source": [
    "#### Pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "179261ae-e168-4035-b9be-45a1f9661c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name, path_levels in zip(['ps_01', 'ps_234', 'ps_5'], [[0.0, 1.0], [2.0, 3.0, 4.0], [5.0]]):\n",
    "    mag_sample_df[col_name] = ''\n",
    "    mag_sample_df.loc[(mag_sample_df.scr_br_0 == True), col_name] = False\n",
    "    mag_sample_df.loc[(mag_sample_df.scr_br_0 == True) & (mag_sample_df.exam_path_severity.isin(path_levels) | mag_sample_df.followup_path_severity.isin(path_levels)), col_name] = True\n",
    "\n",
    "mag_sample_df.loc[mag_sample_df.ps_01 == True, [\"ps_234\", \"ps_5\"]] = False\n",
    "mag_sample_df.loc[mag_sample_df.ps_234 == True, [\"ps_5\"]] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b70ac-9e24-4e61-ae55-7d13f76119aa",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1-6'></a>\n",
    "## 1.6 Identify Interval Cancers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6988959c-48a3-4e43-b4f8-6dacca7fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only consider scr br1/2 exams. \n",
    "# allow bilateral exams with dx path severity in [0.0, 1.0]\n",
    "# allow unilateral exams with dx path severity in [0.0, 1.0] IF the exam laterality and biopsy side match\n",
    "bilat_interval_mask = (mag_sample_df.scr_br_12 == True) & (mag_sample_df.exam_laterality == \"B\") & mag_sample_df.followup_path_severity.isin([0.0, 1.0])\n",
    "unilat_interval_mask = (mag_sample_df.scr_br_12 == True) & (mag_sample_df.exam_laterality != \"B\") & mag_sample_df.followup_path_severity.isin([0.0, 1.0]) & (mag_sample_df.exam_laterality == mag_sample_df.followup_bside)\n",
    "\n",
    "mag_sample_df[\"interval_cancer\"] = ''\n",
    "mag_sample_df.loc[(mag_sample_df.scr_br_12 == True), \"interval_cancer\"] = False\n",
    "mag_sample_df.loc[bilat_interval_mask | unilat_interval_mask, \"interval_cancer\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b9f49-4393-4096-8e31-92647f9b568a",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-1-7'></a>\n",
    "## 1.7 Evaluate Long-Term Follow-Up Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ca10d1c-d055-4c5e-a8b8-b4ca0732ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_condition(df, acc_list, span_list, date_col: str = 'study_date_anon'):\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    mag_contra_df['empi_anon'] = pd.to_numeric(mag_contra_df['empi_anon'])\n",
    "    mag_contra_df['acc_anon'] = pd.to_numeric(mag_contra_df['acc_anon'])\n",
    "    condition_dict = dict()\n",
    "    \n",
    "    # # get a list of all qualifying conditions to do the first pass elim\n",
    "    # qual_list = [span['qualifying'] for span in span_list if span['qualifying'] != \"\"]\n",
    "\n",
    "    # # concatenate all qual strings into an or condition for an initial pass\n",
    "    # init_qual_str = \" | \".join([f\"({qual_str})\" for qual_str in qual_list])\n",
    "    df_acc_list = df.acc_anon.unique().tolist()\n",
    "    acc_list = list(set(acc_list).intersection(set(df_acc_list)))\n",
    "    \n",
    "    # iterate over exams\n",
    "    for target_acc in tqdm(acc_list):\n",
    "        # init bool to track whether the acc has been accepted/rejected\n",
    "        acc_is_valid = True\n",
    "        \n",
    "        # is there a faster way to do this?\n",
    "        acc_mask = df.acc_anon == target_acc\n",
    "        date_i = df.loc[acc_mask, date_col].mode()[0]\n",
    "        empi = df.loc[acc_mask, \"empi_anon\"].mode()[0]\n",
    "\n",
    "        patient_df = df[df.empi_anon == empi]\n",
    "\n",
    "        # iterate over spans\n",
    "        for span in span_list:\n",
    "            # determine span parameters\n",
    "            span_length = span['length'] # throw an error if no span length was given\n",
    "            span_qual = span.get('qualifying', '')\n",
    "            span_disqual = span.get('disqualifying', '')\n",
    "            span_n = span.get('required_n', 0 if not span_qual else 1) # default to 1 if there's a qualifying condition\n",
    "\n",
    "            # get span end date\n",
    "            date_f = (date_i + pd.Timedelta(days=span_length))\n",
    "\n",
    "            # find subset of patient df between the span start/end dates\n",
    "            span_df = patient_df[\n",
    "                (patient_df[date_col] >= date_i)\n",
    "                & (patient_df[date_col] <= date_f)\n",
    "            ]\n",
    "\n",
    "            # evaluate qual condition if it exists, else keep span_df\n",
    "            span_df = span_df.query(span_qual) if span_qual else span_df\n",
    "            \n",
    "            # if span_df < n cases, reject the acc\n",
    "            if span_df.acc_anon.nunique() < span_n:\n",
    "                acc_is_valid = False\n",
    "                break\n",
    "                \n",
    "            # otherwise if we have a qual condition w/ sufficient exams, + a disqual condition to eval\n",
    "            elif span_qual and span_disqual:\n",
    "                # get the study date of the nearest valid exam, and filter span_df to exclude cases later than it\n",
    "                # so we only check the disqual condition up to this point\n",
    "                new_date_f = span_df[date_col].sort_values().unique().tolist()[0]\n",
    "\n",
    "                # reformat the span_disqual string to include the new cutoff date condition\n",
    "                span_disqual += f\" & ({date_col} <= '{new_date_f}')\"\n",
    "                \n",
    "\n",
    "            if span_disqual:\n",
    "                # if there are any disqualifying cases, reject the acc\n",
    "                if span_df.query(span_disqual).acc_anon.nunique():\n",
    "                    acc_is_valid = False\n",
    "                    break\n",
    "            \n",
    "            # increment date_0 before evaluating the next span\n",
    "            # we only need to do this if the last span passes\n",
    "            date_i = (date_f + pd.Timedelta(days=1))\n",
    "\n",
    "        # if all spans pass, accept the acc\n",
    "        if acc_is_valid:\n",
    "            condition_dict[target_acc] = True\n",
    "        else:\n",
    "            condition_dict[target_acc] = False\n",
    "\n",
    "    # return the condition dict after evaluating all exams\n",
    "    return condition_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6e55ed-20e8-43b7-8faf-529074a17fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screen negatives accs: 171422\n",
      "all negatives accs: 201547\n"
     ]
    }
   ],
   "source": [
    "# only evaluate followup status for negative exams\n",
    "scr_neg_acc_list = mag_sample_df.loc[(mag_sample_df.scr_br_12 == True) & (mag_sample_df.interval_cancer == False), 'acc_anon'].unique().tolist()\n",
    "print('screen negatives accs:', len(scr_neg_acc_list))\n",
    "\n",
    "neg_acc_list = mag_sample_df.loc[~((mag_sample_df.scr_br_0 == True) & (mag_sample_df.ps_01 == True)), 'acc_anon'].unique().tolist()\n",
    "print('all negatives accs:', len(neg_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3628ade-8fb2-49e2-89b9-540c7ada8895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171422/171422 [06:13<00:00, 458.66it/s]\n"
     ]
    }
   ],
   "source": [
    "span_list = [\n",
    "    { # the first span has no conditions\n",
    "        'length': 365,\n",
    "        'qualifying': \"\",\n",
    "        'disqualifying': \"\",\n",
    "        'required_n': 0\n",
    "    },\n",
    "    { # the second span accepts accs with >=1 of any exam type\n",
    "        'length': 365*3,\n",
    "        'qualifying': \"\",\n",
    "        'disqualifying': \"\",\n",
    "        'required_n': 1 # required_n applies to the qualifying condition, must be >1 exams of any kind during this period\n",
    "    },\n",
    "]\n",
    "\n",
    "any_followup_dict = verify_condition(mag_contra_df, scr_neg_acc_list, span_list)\n",
    "\n",
    "mag_sample_df['any_followup'] = mag_sample_df.acc_anon.map(any_followup_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c033809-9ba9-499a-930e-71f842ae4e0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-2'></a>\n",
    "# 2. Data Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de44aa5-6b5f-473b-9c59-4c953b95b41a",
   "metadata": {},
   "source": [
    "<a id='section-2-1'></a>\n",
    "## 2.1 Data Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9fb6ded-5e74-4c49-91f8-da89fddea1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_params = embed_toolkit.EMBEDParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b66198-24d1-483c-adb3-3efcdc62e6f1",
   "metadata": {},
   "source": [
    "\n",
    "#### Major Exam Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "495fcf68-fb8d-4384-a8e0-e24e67fd09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample_df[\"label\"] = \"INVALID\"\n",
    "mag_sample_df[\"label\"] = mag_sample_df[\"label\"].case_when([\n",
    "    (mag_sample_df.eval(\"(scr_br_12 == True) & (interval_cancer == False)\"), \"Screen Negative\"), \n",
    "    (mag_sample_df.eval(\"(scr_br_12 == True) & (interval_cancer == True)\"), \"Interval Cancer\"),\n",
    "    (mag_sample_df.eval(\"(scr_br_0 == True) & (ps_01 == True)\"), \"Screen Detected Cancer\"),\n",
    "    (mag_sample_df.eval(\"(scr_br_0 == True) & (ps_234 == True)\"), \"Biopsy Proven Benign\"),\n",
    "    (mag_sample_df.eval(\"(scr_br_0 == True) & (dx_br_123 == True)\"), \"Diagnostic Negative\"),\n",
    "    (mag_sample_df.eval(\"(scr_br_0 == True) & (ps_5 == True)\"), \"Other Cancer\"),\n",
    "])  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306391d9-35a3-47c7-a74c-ee6463a63bb0",
   "metadata": {},
   "source": [
    "#### Finding Characteristics\n",
    "First, we'll derive characteristics for all findings in the sample. Then we'll aggregate these upwards so exam-level characteristics only consider abnormal findings in abnormal exams, and negative/benign findings in negative/benign exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8e08414-c6fa-4ae2-8fba-83ce707d74c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249675/249675 [00:05<00:00, 45074.05it/s] \n"
     ]
    }
   ],
   "source": [
    "# derive findings-level characteristics\n",
    "mag_sample_df[['mass', 'asymmetry', 'arch_distortion', 'calcification']] = mag_sample_df.progress_apply(\n",
    "    embed_params.extract_characteristics, \n",
    "    axis='columns', \n",
    "    result_type='expand'\n",
    ") # type: ignore\n",
    "\n",
    "# finding characteristics should only be present on benign/abnormal findings\n",
    "mag_sample_df.loc[mag_sample_df.asses == \"N\", ['mass', 'asymmetry', 'arch_distortion', 'calcification']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37c250c7-e8a9-4176-a743-f0f1cb36a262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31346/31346 [00:02<00:00, 14385.56it/s]\n",
      "100%|██████████| 171527/171527 [00:12<00:00, 13889.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all exams with a birads A finding and filter them so they only have birads A findings\n",
    "br0_exams = mag_sample_df[mag_sample_df.exam_birads.isin([\"A\"]) & mag_sample_df.asses.isin([\"A\"])]\n",
    "br12_exams = mag_sample_df[mag_sample_df.exam_birads.isin([\"N\", \"B\"])]\n",
    "\n",
    "# generalize to exam-level\n",
    "br0_exam_chars_dict = br0_exams.groupby(\"acc_anon\").progress_apply(embed_params.aggregate_characteristics).to_dict() # type: ignore\n",
    "br12_exam_chars_dict = br12_exams.groupby(\"acc_anon\").progress_apply(embed_params.aggregate_characteristics).to_dict() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0584b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "for char_var in ['exam_mass', 'exam_asymmetry', 'exam_arch_distortion', 'exam_calcification']:\n",
    "    br12_char_dict = {k:v[char_var] for k,v in br12_exam_chars_dict.items()}\n",
    "    br0_char_dict = {k:v[char_var] for k,v in br0_exam_chars_dict.items()}\n",
    "\n",
    "    mag_sample_df[char_var] = mag_sample_df.acc_anon.map({**br12_char_dict, **br0_char_dict})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ee6f089-2996-41aa-81a2-acd660b48db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:00<00:00, 5986.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we need to handle some edge cases in the data that left NaNs in the exam-level characteristics\n",
    "\n",
    "# nan observations are present on cases with invalid BIRADS, and those with adjusted BIRADS N/B \n",
    "# exams that were corrected to BIRADS A exams after inspection for addendums\n",
    "# we'll manually derive the exam-level characteristics for the latter group and set others to False\n",
    "addended_exam_chars_dict = (\n",
    "    mag_sample_df[\n",
    "        pd.isna(mag_sample_df.exam_mass) \n",
    "        & (mag_sample_df.exam_birads == 'A')\n",
    "    ]\n",
    "    .groupby(\"acc_anon\")\n",
    "    .progress_apply(embed_params.aggregate_characteristics) # type: ignore\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# map our addended exam characteristics back to the data\n",
    "for char_var in ['exam_mass', 'exam_asymmetry', 'exam_arch_distortion', 'exam_calcification']:\n",
    "    br12_char_dict = {k:v[char_var] for k,v in br12_exam_chars_dict.items()}\n",
    "    br0_char_dict = {k:v[char_var] for k,v in br0_exam_chars_dict.items()}\n",
    "    addended_char_dict = {k:v[char_var] for k,v in addended_exam_chars_dict.items()}\n",
    "\n",
    "    mag_sample_df[char_var] = mag_sample_df.acc_anon.map({**addended_char_dict, **br12_char_dict, **br0_char_dict})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f4b5681-fc94-4f1f-9251-7ac1f3deec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the only exams with any missing values have invalid BIRADS and will \n",
    "# be dropped, so they can be considered false\n",
    "mag_sample_df = mag_sample_df.fillna(\n",
    "    {\n",
    "        'exam_mass': False, \n",
    "        'exam_asymmetry': False, \n",
    "        'exam_arch_distortion': False, \n",
    "        'exam_calcification': False\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d1828-8d3c-4587-a670-e4f6290763be",
   "metadata": {},
   "source": [
    "#### Screen Detected Pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b78ea8ba-a3bf-4c47-b47e-332ad4ba1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_sample_df[\"scr_detected_path\"] = \"No Pathology\"\n",
    "mag_sample_df.loc[mag_sample_df.scr_br_0 == True, \"scr_detected_path\"] = mag_sample_df.loc[mag_sample_df.scr_br_0 == True, \"scr_detected_path\"].case_when([\n",
    "    (mag_sample_df.eval(\"(exam_path_severity == 0.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 0.0))\"), \"Invasive Cancer\"), \n",
    "    (mag_sample_df.eval(\"(exam_path_severity == 1.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 1.0))\"), \"Noninvasive Cancer\"),\n",
    "    (mag_sample_df.eval(\"(exam_path_severity == 2.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 2.0))\"), \"High Risk Lesion\"),\n",
    "    (mag_sample_df.eval(\"(exam_path_severity == 3.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 3.0))\"), \"Borderline Lesion\"),\n",
    "    (mag_sample_df.eval(\"(exam_path_severity == 4.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 4.0))\"), \"Benign Lesion\"),\n",
    "    (mag_sample_df.eval(\"(exam_path_severity == 5.0) | ((exam_path_severity.isna() | (followup_path_severity < exam_path_severity)) & (followup_path_severity == 5.0))\"), \"Other Cancer\"),\n",
    "]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfddeb-b652-400d-aed9-2afbfe3dea55",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-2-2'></a>\n",
    "## 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421345e-6ec8-4395-bece-9e6b2e0fe2e9",
   "metadata": {},
   "source": [
    "#### Patient Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdf10654-ec8c-4018-80a8-50b3cae0de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize whitespace\n",
    "mag_sample_df['ETHNICITY_DESC'] = mag_sample_df['ETHNICITY_DESC'].str.strip()\n",
    "mag_sample_df['ETHNICITY_DESC'] = mag_sample_df['ETHNICITY_DESC'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "mag_sample_df[\"race\"] = \"Other\"\n",
    "mag_sample_df[\"race\"] = mag_sample_df[\"race\"].case_when([\n",
    "    (mag_sample_df.eval(\"ETHNICITY_DESC == 'African American or Black'\"), \"Black\"),\n",
    "    (mag_sample_df.eval(\"ETHNICITY_DESC == 'Caucasian or White'\"), \"White\"),\n",
    "    (mag_sample_df.eval(\"ETHNICITY_DESC == 'Asian'\"), \"Asian\"),\n",
    "    (mag_sample_df.eval(\"ETHNICITY_DESC == 'Unknown, Unavailable or Unreported'\"), \"Unknown\"),\n",
    "    (mag_sample_df.eval(\"ETHNICITY_DESC.isna()\"), \"Unknown\")\n",
    "]) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef6485-ac12-4222-8b47-4f50e2e68256",
   "metadata": {},
   "source": [
    "#### Patient Ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18857651-32c1-4279-b249-342e4662d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize whitespace\n",
    "mag_sample_df['ETHNIC_GROUP_DESC'] = mag_sample_df['ETHNIC_GROUP_DESC'].str.strip()\n",
    "mag_sample_df['ETHNIC_GROUP_DESC'] = mag_sample_df['ETHNIC_GROUP_DESC'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "hispanic_list = [\"Hispanic or Latino\", \"Unknown~Hispanic\"]\n",
    "not_hispanic_list = [\"Non-Hispanic or Latino\", \"Non-Hispanic~Unknown\", \"Unknown~Non-Hispanic\"]\n",
    "\n",
    "mag_sample_df[\"ethnicity\"] = \"Unknown\"\n",
    "mag_sample_df[\"ethnicity\"] = mag_sample_df[\"ethnicity\"].case_when([\n",
    "    (mag_sample_df.eval(\"ETHNIC_GROUP_DESC.isin(@hispanic_list)\"), \"Hispanic or Latino\"),\n",
    "    (mag_sample_df.eval(\"ETHNIC_GROUP_DESC.isin(@not_hispanic_list)\"), \"Not Hispanic or Latino\"),\n",
    "]) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839498fa-23cd-4a82-88bb-f24ec983afa2",
   "metadata": {},
   "source": [
    "#### Patient Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69347e45-d6b4-4888-82ed-5e3c5b5014c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_binned\n",
       "50-75    133966\n",
       "<50       49988\n",
       ">=75      18969\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mag_sample_df['age_binned'] = pd.cut(mag_sample_df['age_at_study'], bins=[0, 50, 75, 120], labels=[\"<50\", \"50-75\", \">=75\"])\n",
    "mag_sample_df.drop_duplicates('acc_anon')['age_binned'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6056d4-2a0d-4b41-8b41-43752657c3f9",
   "metadata": {},
   "source": [
    "<a href='#contents'>\n",
    "    <button style='margin-top: 1rem; padding: 0.5rem; cursor: pointer;'>Back to Top</button>\n",
    "</a>\n",
    "\n",
    "<a id='section-2-3'></a>\n",
    "## 2.3 Finalize and Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6041d726-d851-4ae9-9cbe-ade4fe2144f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure tissueden is numeric\n",
    "mag_sample_df['tissueden'] = pd.to_numeric(mag_sample_df['tissueden'], errors=\"coerce\")\n",
    "\n",
    "# get the accessions of any exams with invalid screening-stage BIRADS\n",
    "dx_only_asses = [\"P\", \"S\", \"M\", \"K\"]\n",
    "invalid_exam_list = (\n",
    "    mag_sample_df.loc[\n",
    "        mag_sample_df.desc.str.contains(\"screen\", case=False) \n",
    "        & mag_sample_df.asses.isin(dx_only_asses), \n",
    "        'acc_anon'\n",
    "    ]\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abf6e9f8-c294-49a1-974b-77843cc63437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any cases where tissueden is 5.0, with no valid major label, or with other cancers present\n",
    "final_df = mag_sample_df[\n",
    "    ~((mag_sample_df.tissueden == 5.0)\n",
    "      | (mag_sample_df.acc_anon.isin(invalid_exam_list))\n",
    "      | (mag_sample_df.label == \"INVALID\") \n",
    "      | (mag_sample_df.label == \"Other Cancer\") \n",
    "      | (mag_sample_df.scr_detected_path == \"Other Cancer\")\n",
    "      | (mag_sample_df.any_followup == False))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6563ac0-e063-4949-9926-fbc8b19d2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_PATH is not None:\n",
    "    final_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"data saved to: '{OUTPUT_PATH}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28278b9a-59e1-4bf6-b17c-2f4fc8533cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "fontLicenses": {},
   "fonts": {},
   "styles": {
    ":root": {}
   }
  },
  "kernelspec": {
   "display_name": "dsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
